---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
  namespace: {{ .Values.username }}-llama-stack
data:
  run.yaml: |
    # Llama Stack configuration
    version: '2'
    image_name: vllm
    apis:
    - inference
    - tool_runtime
    - agents
    - safety
    - vector_io
    models:
      - metadata: {}
        model_id: {{ .Values.defaultModel.name }}
        provider_id: vllm-{{ .Values.defaultModel.name }}
        provider_model_id: {{ .Values.defaultModel.name }}
        model_type: llm
    providers:
      inference:
      - provider_id: vllm-{{ .Values.defaultModel.name }}
        provider_type: "remote::vllm"
        config:
          url: {{ .Values.defaultModel.url }}
          context_length: 4096
          api_token: ${env.DEFAULT_MODEL_API_TOKEN}
          tls_verify: true
      tool_runtime:
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db
    server:
      port: 8321
    tools:
      - name: builtin::websearch
        enabled: true
    tool_groups:
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
